# Лабораторная работа №2

**Тема:** Работа с предобученными моделями и эмбеддингами  
**Цель:** Научиться использовать предобученные трансформеры и исследовать влияние различных слоев модели.

**Студентs:** Чупахина В.В., Фомичева П.Ю.

---

## Содержание

1. [Теоретическая часть](#теоретическая-часть)  
2. [Описание разработанной системы (алгоритмы, принципы работы, архитектура)](#описание)
3. [Результаты](#результаты)  
4. [Выводы](#выводы)  
5. [Использованные источники](#использованные-источники)  

---

## Теоретическая часть

### 1. Архитектура трансформера

Трансформер — это архитектура нейронных сетей, предложенная в работе *"Attention is All You Need"* [1]. В отличие от RNN, трансформер использует механизм **самовнимания (self-attention)**, который позволяет модели учитывать весь контекст входной последовательности одновременно и эффективно.

#### Основные компоненты трансформера:
- **Self-Attention** — механизм, который позволяет каждому токену обращать внимание на другие токены в последовательности.
- **Multi-Head Attention** — параллельные головы внимания, извлекающие разные типы связей.
- **Feed-Forward Network** — полносвязная сеть, применяемая к каждому токену отдельно.
- **Residual Connections** и **Layer Normalization** — улучшают сходимость модели и стабильность обучения.

---

### 2. Модель RoBERTa

**RoBERTa** (*Robustly Optimized BERT Pretraining Approach*) — это модифицированная и улучшенная версия модели BERT, представленная исследователями из Facebook AI [2].

Ключевые отличия от BERT:
- Обучение на существенно большем объёме данных.
- Исключение задачи Next Sentence Prediction.
- Использование динамического маскирования токенов при обучении.
- Увеличенное время и объём предобучения.

Эти изменения делают RoBERTa более точной и устойчивой моделью для задач обработки естественного языка.

---

### 3. Эмбеддинги в трансформерах

**Эмбеддинг** — это способ представления текста в виде плотных векторов фиксированной размерности. В трансформерах используется несколько видов эмбеддингов:

- **Token Embeddings** — представление слов или подслов.
- **Positional Embeddings** — добавляют информацию о позиции токена в последовательности.
- **Hidden States** — векторы, полученные на выходе каждого слоя трансформера.
- **[CLS] эмбеддинг** — вектор специального токена `[CLS]`, часто используемый для классификации текста как агрегированное представление всей последовательности.

---

### 4. Поведение слоев трансформера

Исследования показывают, что различные слои трансформера специализируются на разных аспектах лингвистической информации:

- **Нижние слои** хорошо захватывают **морфологические и синтаксические** структуры.
- **Средние слои** содержат **семантическую** информацию.
- **Верхние слои** подстроены под задачу обучения (например, предсказание маскированных токенов) и могут быть менее универсальными [3][4].

---

### 5. Извлечение эмбеддингов

С помощью библиотеки HuggingFace можно получить скрытые состояния с любого слоя модели:

```python
from transformers import RobertaModel, RobertaTokenizer

model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

inputs = tokenizer("Пример текста", return_tensors="pt")
outputs = model(**inputs)

hidden_states = outputs.hidden_states  # Tuple из 13 тензоров (включая embedding слой)
