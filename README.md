# Лабораторная работа №2

**Тема:** Работа с предобученными моделями и эмбеддингами  
**Цель:** Научиться использовать предобученные трансформеры и исследовать влияние различных слоев модели.

**Студенты:** Чупахина В.В., Фомичева П.Ю., Салимова А.Ф.

---

## Содержание

1. [Теоретическая часть](#теоретическая-часть)  
2. [Описание разработанной системы (алгоритмы, принципы работы, архитектура)](#описание-разработанной-системы-алгоритмы-принципы-работы-архитектура)
3. [Результаты](#результаты)  
4. [Выводы](#выводы)  
5. [Список использованных источников](#список-использованных-источников)  

---

## Теоретическая часть

### Архитектура трансформера

Трансформер — архитектура глубоких нейронных сетей, представленная в 2017 году исследователями из Google Brain.

По аналогии с рекуррентными нейронными сетями (РНС) трансформеры предназначены для обработки последовательностей, таких как текст на естественном языке, и решения таких задач как машинный перевод и автоматическое реферирование. В отличие от РНС, трансформеры не требуют обработки последовательностей по порядку. Например, если входные данные — это текст, то трансформеру не требуется обрабатывать конец текста после обработки его начала. Благодаря этому трансформеры распараллеливаются легче чем РНС и могут быть быстрее обучены[1].

Архитектура трансформера (рисунок 1) состоит из кодировщика и декодировщика. Кодировщик получает на вход векторизованую последовательность с позиционной информацией. Декодировщик получает на вход часть этой последовательности и выход кодировщика. Кодировщик и декодировщик состоят из слоев. Слои кодировщика последовательно передают результат следующему слою в качестве его входа. Слои декодировщика последовательно передают результат следующему слою вместе с результатом кодировщика в качестве его входа.

Каждый кодировщик (рисунок 2) состоит из механизма самовнимания (вход из предыдущего слоя) и нейронной сети с прямой связью (вход из механизма самовнимания). Каждый декодировщик (рисунок 3) состоит из механизма самовнимания (вход из предыдущего слоя), механизма внимания к результатам кодирования (вход из механизма самовнимания и кодировщика) и нейронной сети с прямой связью (вход из механизма внимания) [1].

<p align="center">  
  <img src="https://upload.wikimedia.org/wikipedia/commons/0/0d/MLTransformerOverview.svg">
</p>

<p align="center">  
Рисунок 1 - Архитектура трансформера
</p>

<p align="center">  
  <img src="https://github.com/user-attachments/assets/50db7e21-d1ea-4098-856e-0bab8423df54">
</p>

<p align="center">  
Рисунок 2 - Кодировщик
</p>

<p align="center">  
    <img src="https://github.com/user-attachments/assets/06d4d53a-481d-4634-b68a-b8bf02753732">
</p>

<p align="center">  
Рисунок 3 - Декодировщик
</p>

Прежде чем подать данные в трансформер, нужно сначала преобразовать их в последовательность токенов — набор целых чисел, представляющих входные данные.
В нём слова должны быть сопоставлены с числами. Можно зарезервировать определенное число для представления любого слова, которого нет в словаре, поэтому мы сможем всегда оперировать целыми числами [2].

---

### Эмбеддинг

В широком смысле, эмбеддинг - это процесс преобразования каких-либо данных (чаще всего текста, но могут быть и изображения, звуки и т.д.) в набор чисел, векторы, которые машина может не только хранить, но и с которыми она может работать.
Эмбеддинг - это способ преобразования чего-то абстрактного, например слов или изображений в набор чисел и векторов. Эти числа не случайны; они стараются отражают суть или семантику нашего исходного объекта.

В NLP, например, эмбеддинги слов используются для того, чтобы компьютер мог понять, что слова «кошка» и «котенок» связаны между собой ближе, чем, скажем, «кошка» и «окошко». Это достигается путем присвоения словам векторов, которые отражают их значение и контекстное использование в языке.

Эмбеддинги не ограничиваются только словами. В компьютерном зрении, например, можно использовать их для преобразования изображений в вектора, чтобы машина могла понять и различать изображения [3].

**Векторные пространства** — это математические структуры, состоящие из векторов. Векторы можно понимать как точки в некотором пространстве, которые обладают направлением и величиной. В эмбеддингах, каждый вектор представляет собой уникальное представление объекта, преобразованное в числовую форму.

**Размерность вектора** определяет, сколько координат используется для описания каждого вектора в пространстве. В эмбеддингах высокая размерность может означать более детализированное представление данных. Векторное пространство для текстовых эмбеддингов может иметь тысячи измерений.

**Расстояние между векторами** в эмбеддингах измеряется с помощью метрик, таких как Евклидово расстояние или косинусное сходство. Метрики позволяют оценить, насколько близко или далеко друг от друга находятся различные объекты в векторном пространстве, что является основой для многих алгоритмов машинного обучения, таких как классификация

**В общем виде, эмбеддинги делятся на [3]:**

| Категория  | Тип | Описание |
| ------------- | ------------- | ------------- |
| Текстовые эмбеддинги  | Word Embeddings  | Эти эмбеддинги преобразуют слова в векторы, так что слова с похожим значением имеют похожие векторные представления. Они впервые позволили машинам понять семантику человеческих слов.  |
| Текстовые эмбеддинги  | Sentence Embeddings  | Здесь уже идет дело о целых предложениях. Подобные модели создают векторные представления для целых предложений или даже абзацев, улавливая гораздо более тонкие нюансы языка.  |
| Эмбеддинги изображений  | CNN  | CNN позволяет преобразовать изображения в векторы, которые затем используются для различных задач, например, классификации изображений или даже генерации новых изображений.  |
| Эмбеддинги изображений  | Autoencoders  | Автоэнкодеры могут сжимать изображения в более мелкие, плотные векторные представления, которые затем могут быть использованы для различных целей, включая декомпрессию или даже обнаружение аномалий.  |
| Эмбеддинги для других типов данных  | Graph Embeddings  | Применяются для работы с графовыми структурами (к примеру рекомендательные системы). Это способ представить узлы и связи графа в виде векторов.  |
| Эмбеддинги для других типов данных  | Sequence Embeddings  | Используются для анализа последовательностей, например, во временных рядах или в музыке.  |

### Основные алгоритмы
Word2Vec
Word2Vec использует нейронные сети для обучения векторных представлений слов из больших наборов текстовых данных. Существуют две основные архитектуры Word2Vec:

CBOW: предсказывает текущее слово на основе контекста (окружающих слов). Например, в предложении "Собака лает на ___", CBOW попытается угадать недостающее слово (например, "почтальона") на основе окружающих слов.

Skip-gram: работает наоборот по сравнению с CBOW. Использует текущее слово для предсказания окружающих его слов в предложении. Например, если взять слово "кошка", модель попытается предсказать слова, которые часто встречаются в окружении слова "кошка", такие как "мышь", "мяукает" и т.д.

---

## Описание разработанной системы (алгоритмы, принципы работы, архитектура)

### Модель RoBERTa

**RoBERTa** (*Robustly Optimized BERT Pretraining Approach*) — это модифицированная и улучшенная версия модели BERT. Она сохраняет архитектуру BERT, но обучается по другим принципам, благодаря которым достигает более высоких результатов на множестве задач.

Ключевые отличия от BERT:
- Обучение на существенно большем объёме данных.
- Исключение задачи Next Sentence Prediction.
- Использование динамического маскирования токенов при обучении.
- Увеличенное время и объём предобучения.

RoBERTa использует метод маскированного языкового моделирования (MLM), где случайные токены во входном тексте заменяются специальным символом <mask>, а задача модели — предсказать исходные токены. Это обучение позволяет модели захватывать контекст слов в обоих направлениях — слева и справа.

В процессе обработки текста модель сначала превращает слова в токены с помощью алгоритма Byte-Pair Encoding (BPE). Затем каждый токен преобразуется в векторное представление (эмбеддинг) с помощью обученной модели. Итоговый эмбеддинг для слова или предложения — это вектор фиксированной размерности (например, 768 или 1024), который можно использовать в различных NLP-задачах: классификации текста, извлечении сущностей, машинном переводе и т.д [4].

**Особенности эмбеддингов RoBERTa:**

 - Учитывают контекст: одно и то же слово в разных предложениях может иметь разные векторные представления.
 - Обеспечивают более высокую точность по сравнению с BERT за счёт лучшей стратегии обучения.
 - Не используют NSP (Next Sentence Prediction), в отличие от оригинального BERT.

Эмбеддинги RoBERTa можно извлекать с помощью библиотек, таких как HuggingFace Transformers, и применять в задачах анализа текста, где важна семантическая интерпретация.

**Типы эмбеддингов в RoBERTa**

 - Токен-эмбеддинги (token embeddings) — представления отдельных токенов. Эти векторы учитывают контекст, в котором находится слово.
 - Позиционные эмбеддинги (positional embeddings) — добавляются к токен-эмбеддингам, чтобы учитывать порядок слов в предложении.
 - Сегментные эмбеддинги в RoBERTa не используются, так как модель не оперирует задачей предсказания следующего предложения.

**Последовательность формирования эмбеддинга**

 - Токенизация — исходный текст разбивается на подслова (субтокены) с помощью BPE.
 - Встраивание — каждый токен преобразуется в начальный эмбеддинг (вектор).
 - Сложение с позиционными эмбеддингами — чтобы модель знала порядок слов.
 - Пропуск через слои трансформера — векторы проходят через множество самовнимательных слоёв, формируя финальные эмбеддинги.

## Результаты

В результате выполнения лабораторной работы были получены эмбеддинги Robert и построена нейронная сеть на их основе. 
Оценка качества модели на тесте (Precision/Recall/F1Score) приведена на рисунке 4.

<p align="center">  
  <img src="https://github.com/user-attachments/assets/de304ff1-8d9a-4e4b-a07f-80c7903e5825">
</p>

<p align="center">  
Рисунок 4 - Оценка качества модели
</p>

Результаты обучения модели представлены на рисунке 5.

![image](https://github.com/user-attachments/assets/09f17d76-774b-4c5c-ac7b-db41aa711312)

<p align="center">  
  <img src="https://github.com/user-attachments/assets/1f3b5caf-ced3-4849-9b5f-f0b5bb6bebf6">
</p>

<p align="center">  
Рисунок 5 - Результаты
</p>


## Выводы

В результате выполнения лабораторной работы мы научились работать с предобученными моделями и на основе предобученных эмбеддингов научились строить новые модели.
**Основные выводы по работе:**

Предобученные эмбеддинги RoBERTa позволяют эффективно извлекать контекстуальные векторные представления текста, что повышает качество моделей в задачах NLP.

В отличие от классических эмбеддингов (Word2Vec, FastText), модель RoBERTa учитывает положение слов в контексте и обеспечивает более точное представление их значения.

В экспериментах модель показала accuracy = 84%, сбалансированную по метрикам precision, recall и F1-score: F1-score: 0.83 для класса short, 0.85 для long

macro и weighted средние значения также составляют 0.84

Это говорит о том, что модель хорошо обобщается, не переобучается под один из классов и даёт устойчивое качество классификации.

## Список использованных источников

1.  Википедия : сайт - URL: https://ru.wikipedia.org/wiki/%D0%A2%D1%80%D0%B0%D0%BD%D1%81%D1%84%D0%BE%D1%80%D0%BC%D0%B5%D1%80_(%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D1%8F) (дата обращения: 08.04.2025).

2. Habr : сайт - URL: https://habr.com/ru/companies/mws/articles/770202/ (дата обращения: 08.04.2025).
   
3. Habr : сайт - URL: https://habr.com/ru/companies/otus/articles/787116/ (дата обращения: 08.04.2025).
   
4. Liu, Yinhan et al. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692.
https://arxiv.org/abs/1907.11692

