# Лабораторная работа №2

**Тема:** Работа с предобученными моделями и эмбеддингами  
**Цель:** Научиться использовать предобученные трансформеры и исследовать влияние различных слоев модели.

**Студентs:** Чупахина В.В., Фомичева П.Ю.

---

## Содержание

1. [Теоретическая часть](#теоретическая-часть)  
2. [Описание разработанной системы (алгоритмы, принципы работы, архитектура)](#описание-разработанной-системы-алгоритмы-принципы-работы-архитектура)
3. [Результаты](#результаты)  
4. [Выводы](#выводы)  
5. [Использованные источники](#использованные-источники)  

---

## Теоретическая часть

### Архитектура трансформера

Трансформер — архитектура глубоких нейронных сетей, представленная в 2017 году исследователями из Google Brain.

По аналогии с рекуррентными нейронными сетями (РНС) трансформеры предназначены для обработки последовательностей, таких как текст на естественном языке, и решения таких задач как машинный перевод и автоматическое реферирование. В отличие от РНС, трансформеры не требуют обработки последовательностей по порядку. Например, если входные данные — это текст, то трансформеру не требуется обрабатывать конец текста после обработки его начала. Благодаря этому трансформеры распараллеливаются легче чем РНС и могут быть быстрее обучены[1].

Архитектура трансформера (рисунок 1) состоит из кодировщика и декодировщика. Кодировщик получает на вход векторизованую последовательность с позиционной информацией. Декодировщик получает на вход часть этой последовательности и выход кодировщика. Кодировщик и декодировщик состоят из слоев. Слои кодировщика последовательно передают результат следующему слою в качестве его входа. Слои декодировщика последовательно передают результат следующему слою вместе с результатом кодировщика в качестве его входа.

Каждый кодировщик состоит из механизма самовнимания (вход из предыдущего слоя) и нейронной сети с прямой связью (вход из механизма самовнимания). Каждый декодировщик состоит из механизма самовнимания (вход из предыдущего слоя), механизма внимания к результатам кодирования (вход из механизма самовнимания и кодировщика) и нейронной сети с прямой связью (вход из механизма внимания).


![image](https://upload.wikimedia.org/wikipedia/commons/0/0d/MLTransformerOverview.svg)

<p align="center">  
Рисунок 1 - Архитектура трансформера
</p>

---

### Модель RoBERTa

**RoBERTa** (*Robustly Optimized BERT Pretraining Approach*) — это модифицированная и улучшенная версия модели BERT, представленная исследователями из Facebook AI [2].

Ключевые отличия от BERT:
- Обучение на существенно большем объёме данных.
- Исключение задачи Next Sentence Prediction.
- Использование динамического маскирования токенов при обучении.
- Увеличенное время и объём предобучения.

Эти изменения делают RoBERTa более точной и устойчивой моделью для задач обработки естественного языка.

---

### Эмбеддинги в трансформерах

**Эмбеддинг** — это способ представления текста в виде плотных векторов фиксированной размерности. В трансформерах используется несколько видов эмбеддингов:

- **Token Embeddings** — представление слов или подслов.
- **Positional Embeddings** — добавляют информацию о позиции токена в последовательности.
- **Hidden States** — векторы, полученные на выходе каждого слоя трансформера.
- **[CLS] эмбеддинг** — вектор специального токена `[CLS]`, часто используемый для классификации текста как агрегированное представление всей последовательности.

---

### Поведение слоев трансформера

Исследования показывают, что различные слои трансформера специализируются на разных аспектах лингвистической информации:

- **Нижние слои** хорошо захватывают **морфологические и синтаксические** структуры.
- **Средние слои** содержат **семантическую** информацию.
- **Верхние слои** подстроены под задачу обучения (например, предсказание маскированных токенов) и могут быть менее универсальными [3][4].

---
## 2. Описание разработанной системы (алгоритмы, принципы работы, архитектура)

## 3. Результаты

## 4. Выводы

## 5. Список использованных источников

